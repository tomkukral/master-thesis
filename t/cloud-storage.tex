%pdflatex-../thesis.tex
% vim:spell spelllang=en_us

% Storage

Storage is an essential part of datacenter since it provides space for saving information. We can distinguish between different types of storages and dozens of storage access methods so it is obvious that building one universal solution for all use cases is impossible. 

\subsection{Physical storage}
Storage solution can be divided into layers. First layer is physical storage and it is represented by physical hardware used to save data onto. It is for example physical rotary drive, \Ac{SSD}, tape drive or any other kind of physical storage. Every server can be equipped with smaller drive and use it is uses for services running on this node. This approach is called Direct Attached Storages and it is connected with share-nothing architecture. The advantages of share-nothing is high level of node's independence, but I think that this solution is not flexible enough to be used as main storage scenario for whole data center.

Another way of providing servers with semi-physical storage is using some sort of shared storage. There is a box stuffed with physical storage and other servers are using this storage via some standardized protocol like \Ac{FCP} or \Ac{iSCSI}. Shared storage brings better flexibility compared to direct attached storage, because physical storage is not fixed to a single server. However shared storages is usually centralized too and it can bring bottleneck or single point of failure.

Physical storage can be organized into layers called tiers. Tiers are groups of physical storage devices usually organized by their performance, reliability or price. It is obviously better to uses only high performance and reliable devices, however these are usually the most expensive ones with low capacity/price coefficient. Tiered storage can provide high performance and high capacity together but it is necessary to use different tiers and optimize data placement. Storage with 3 tiers can use following drives:
\begin{itemize}
	\item tier 1 - \Ac{FC} drives
	\item tier 2 - \Ac{SATA} drives
	\item tier 3 - magnetic tapes
\end{itemize}
Exactly this 3 tier model is used by CESNET's data storages facilities in Pilsen. However tier model can be extended with more technologies such as flashcache\footnote{\url{http://github.com/facebook/flashcache}} capable of increasing drive \Ac{IOPS} by writethrough caching to another faster drive. Another extension layers may be introduced by using different \Ac{RAID} for every tier.

It is obviously not possible to share physical storage in distributed datacenter but semi-physical storage (like \Ac{iSCSI}) is technically possible to share. However latency can be serious problem, because a few millisecond can be significant increase to disk \Ac{IO} operations. For example \Ac{RTT} from server located in Czech republic to Norway (Trondheim) is about $50~\Jed{ms}$ and $150~\Jed{ms}$ to USA (Stanford). This time is not acceptable for sharing physical storage, because each disk access time performed in distributed storage will be about $100~\Jed{ms}$ longer than physical access time. It may be possible to use this kind of storage of longer distances using direct optical connection with minimal \Ac{RTT}, like CESNET Lambda or Photonic.

\subsection{Virtual storage}
%It is possible pro build separate network dedicated to storage, called \Ac{SAN}, or use existing data network. Isolated \Ac{SAN} can provide better performance but it is obviously more expensive to build. However using convergent network for storage is not as straight solution as it may look because it is necessary to deploy advanced technologies like lossless Ethernet.

To deliver storage for physical server is only the first problem, since it is required to split this raw storage and use for another storage layers. There are three fundamental virtual storage types: block storage, file system and object storage. It does not matter if this virtual storage is used directly by physical server or by virtual server via any virtualization layer since access method is actually the same.

% block 
Block storage is used way similar to directly attached. Block device is exposed to operating system and storage is accessed through block device. Storage operations are managed directly by the operating system and it is main characteristic of this storage type. Block device can be formated with filesystem, used as physical volume for \Ac{LVM} or used as encrypted storage.

% filesystem
Filesystem storage is used to save files and it's attributes. Filesystem can be build upon local block device or accessed as \Ac{NAS}. Filesystem on local block device can deliver lower latency and it is easier to match permissions with local user accounts, but these advantages count only for strictly local filesystem. I think that using \Ac{NAS} is almost required in case when any between user or computer sharing is required. Storage accessed via network will probably be easier to maintain and it is also easier to perform backups. 

% object storage
Object storage is something between previously mentioned storage types since it is capable to save objects and access them similar way as accessing files in filesystem. However object storage is more general way of saving data and can be used to store various objects and it is not limited only to files. It provides very high level of storage abstraction because it is capable to operate on almost any data, access it in standardized way and decouples object from their location.
\label{par:object-storage}

%It is even possible to build filesystem upon object storage and it is used for example in CephFS\footnote{\url{https://ceph.com/docs/master/cephfs/}}. This solution uses two pools of objects for \Uv{emulation} filesystem. Files are saved into ono pool as objects and related metadata are save in other pool. 

\subsection{Network access}
I think that traditional approach with separate block and file storage is becoming nowadays quite limiting. There are some request which are partially mutually exclusive:
\begin{description}
	\item[redundancy and distribution] to spread load and fail resistance
	\item[agility and scalabity] to provide flexible and elastic storage
	\item[security] because it is critical to avoid any unauthorized access or leakage
	\item[inteoperabilty] between different technologies and vendors
\end{description}

Distributed storages is a solution which can fulfill all of these requirements. This kind of storage can not referred as neither block nor storage, because it is necessary to use different storage element. These elements are stored on storage nodes according to storage map and rules. It is possible to define how many time should be each element saved, on how many node or on which type of physical storage.

Element mentions in paragraph above can be easily referred as an object, so object storage can be build in distributed way. However not every object storage is distributed storage, because object storages can be local too.

Typical example of distributed object storage is Ceph, referred in \cite{ceph}. It uses Reliable, Automatic, Distributed Object Store (\Ac{RADOS}) mechanism to store object in Ceph cluster. Every type of data is stored as an object in flat namespace so it doesn't matter whether it is text, file or binary image. 

Common problem of distributes storages is central gateway, which is used for client connection and coordinates whole cluster, so it introduces single point of failure. However Ceph eliminates this by using \Ac{CRUSH} algorithm to compute object location without querying central lookup table. Each client can compute object placement on it's own and then directly connect to storage node.

Cluster build with Ceph consist of two types of nodes \Ac{OSD} and monitor. \Ac{OSD} is used to store objects and monitor is responsible for maintaining placement map and monitoring of other monitors and \Ac{OSD}. It is necessary to design cluster well because performance can be poor otherwise. It is, for example, not recommended to place \Ac{OSD} and monitor on one disk, because many parallel \Ac{IO} operations will be requested.

Object store is base and it can be extended by other services, there is for example \Ac{RBD} providing block devices and CephFS used to store files. Distributed object storage is well suited for distributed datacenter because it can be designed to provide distributed shared storage. However it introduces additional abstraction layer in storage system, so it will probably provide worse performance compared to strictly physical storage. It is tradeoff for flexibility.




