%pdflatex-../thesis.tex
% vim:spell spelllang=en_us

% Networking

Networking is essential part of cloud computing because it is not be possible to access any services without networking. Every service in cloud computing system is accessed via network. Network is usually also used for communication between virtual machines, migrations, storage access and for many other tasks.


% L1
First part of networking to think about is physical layer. Various Ethernet versions are used for physical layer in cloud data centers. There are many versions with different link bandwidth and wiring, but 1G and 10G with twisted pairs or optical fibers is used most widely. There were 100M called Fast Ethernet but it does not make sense to use this for server link today, because of it's limited bandwidth and almost similar price compared to 1G.

% server
It is common to insert two independent \Ac{NIC}s into every server and connect them into independent \Ac{ToR} switches, because it improves fault-tolerance. There is usually one more \Ac{NIC} for remote management and some additional cards if \Ac{SAN} is used. Remote management can be connected with detached cable or shared with any of network cards. Separate cable brings more flexibility and fault-tolerance and shared cable reduces cabling effort and thus simplifies maintenance and improves cooling effectiveness. Both solutions are used. 

% rack & ToR
About 40 servers fits into traditional rack and these server need to be connected to network infrastructure. There are different topologies, but most common is variant of (fat) tree with top-of-rack switch. There is also approach called \Uv{fabric} which implies non-blocking every-to-every mesh connection between switches. However fabric technologies are proprietary and limited to vendor.

Every rack contains about 40 servers and these servers are connected to switch called \Ac{ToR}. This switch is located in the rack and acts as access layer for servers. Servers are connected to at least two \Ac{ToR}s if additional fault-tolerance is needed. Upper network topology layers depends on data center size and scaling requirements. There can be distribution and core layer, collapsed core or some kind of fabric.

Physical topology must be adjusted to spread network layers between two or more datacenter networks. It is obviously not possible to spread physical layer, but data link layer and upper layers are possible to spread.

% L3 & address
Another view on network topology is at network layer. Internet is based on \Ac{TCP}/\Ac{IP} so it is necessary to use this protocol family and assign \Ac{IP} addresses to servers, virtual machines and other network elements. There are two different versions of \Ac{IP} protocol:
\begin{description}
	\item[version 4] is the older one, with 32 bit address space. This version is still used more than version 6 even though it's address space is depleted and new version exists for more than 15 years.
	\item[version 6] is the \Uv{new} one, uses 128 bit address space and different headers, thus it is incompatible with version 4.
\end{description}

Moder data center must provide both versions of \Ac{IP} protocol, because supporting only one versions is a huge limitation and can not be accepted for new services deployment. 
However there is a problem with obtaining \Ac{IPv4} addresses, because available pool had already been depleted and all available addreses had been divided between \Ac{RIR}s. It is beneficial to make efforts to employ \Ac{IPv6} protocol as primary one and try to limit the amount of required \Ac{IPv4} addresses.


There are different ways how to use both versions concurently:
\begin{itemize}
	\item Dual-stack is the simpliest and probably the most used solution. Each interface gets at least one \Ac{IPv4} and one \Ac{IPv6} address. Use of both versions causes additional maintenance effort because it is necessary to take care of two separate L3 networks.
	\item Tunelling \Ac{IPv6} via existing \Ac{IPv4} infrastructure with technologies like 6to4, 6rd or \Ac{ISATAP} is another way. This solution can be used for \Ac{IPv6} deployment in networks with working \Ac{IPv4}, because it is used for transmission of all packets. 
	Tunelling is usually focused on deployment in access networks, but deployment in data center network is also applicable, as described in \cite{draft-sakura-6rd}.
	\item Translating \Ac{IPv4} addresses into part of \Ac{IPv6} addressing space is different approach than previous mentinoned, because it operates on \Ac{IPv6}-only networks. This technique does not require every box to have assigned an \Ac{IPv4} address and thus is good for saving address space. Hovewer address translations may not be suitable for data center usage, because it does not preserve original \Ac{IP} addresses and makes customer tracking almost impossible. Further information can be found in \cite{ipv4-jako-sluzba}.
\end{itemize}

It is beneficial to deploy protocol \Ac{IPv6} as primary one in my opinion, because it will become more and more needed during time. Hardware on current market usually support protocol \Ac{IPv6} at least partialy, but there are still some hidden pitfalls. There may be problem for example with server's remote management, because it may not support \Ac{IPv6} and thus is totally unusable on \Ac{IPv6}-only network. None of servers I have used for practical part of this thesis have support for \Ac{IPv6} on \Ac{IPMI}. 
However there is currently only small demand and \Ac{IPv6} deployment does not bring any direct profit. Even thought there are many problems and advantages are quite hidden, it is not possible to ignore this protocol and stay with old \Ac{IPv4}.

Virtual machine migration must be taken into account durring addressing schema design, because it plays crucial role in data center operation. Migrations are performed between hypervisors, i.e. physical servers, and these servers may be located in different racks, halls or even in different data centers. It is usually required to preserve \Ac{IP} address of virtual machine during migration process and thus adressing schema must be prepared to move single \Ac{IP} address around almost whole data center without any significant configuration changes. 

% shared L2
First solution for unlimited migrations while preserving \Ac{IP} address is L2 sharing between hypervisors. Data link layer is shared between all hypervisors and then virtual machine is located in same L2 network before and after migration. This solution does not scale well since it is not recomended to place more than a few hundreds of hosts into single L2 domain, so it may be necessary to divide single big L2 into many smaller networks. It is quite easy to employ this solution for hypervisors in same rack, but it is more difficult with more distant servers and even more when servers are located at different data centers.

% routing 
Another way how to accomplish unlimited \Ac{VM} migrations is to use routinl for machine connectivity. This method uses temporary and fixed \Ac{IP} addresses. Hypervisor do not have to be in same L3 network and there is higher variability in addresses assigned to virtual machines. Temporary address is assigned according to \Ac{VM}'s location and it changes during migration. Fixed address is routed to \Ac{VM} and this address does not change, because changes are made only in routing tables. Any routing protocol can be usel, e.g. \Ac{OSPF}, to provide correct routing of fixed address destination virtual machine. Is is neccesary to insert one record in routing table for every virtual machine, because \textbackslash 32 routes are being advertised. Huge routing table may be problem for data centers with many virtual machines.
Higher layer is used to get more flexibility than lower level can offer. Main drawback of this solution is additinal complexity caused by routing and longer address swap, because it takes some time to propagate routing to new temporary address. It is not easy to perform live migration, because it is neccesary to change \Ac{IP} address of \Ac{VM}'s interface and thus open sessions will terminate.

\subsection{Overlays}
Virtualization is used heavily these days, therefore it is necessary provide networking solution with at least same flexibility as virtualization offers. Multitenancy, \Ac{VM} migrations, fast reconfiguration and rapid deployment are most missing features of physical networks these days. It is currently possible to migrate virtual machines without interruption of services, but it is quite difficult to carry out same task on network layer. Overlay networking is one of proposed solutions and it is supposed to bring additional abstraction layer capable of decoupling network from physical hardware. 
Technologies capable to build overlay network are for example \Ac{VXLAN}, \Ac{STT} and \Ac{NVGRE}. 

There is usually used any kind of encapsulation for building overlay network on top of physical one. Encapsulation of course have drawbacks in higher computations demand and packet size problems, but advantages brought by overlay are appreciable:
\begin{itemize}
	\item Configuration of physical network is left untouched, so configuration fails are minimized
	\item Overlay network is only virtual and more flexible than physical network
	\item It is easy to adjust overlay network since it is only done in software
	\item Tenant isolation can be done better because overlays are designed to support this
\end{itemize}

\subsubsection{VXLAN}
\subsubsection{NVGRE}
\subsubsection{STT}

\subsection{Storage network}
% storage network

\subsection{Load balancing}
% load balancing

\subsection{Firewall}
% firewall 

\subsection{Software Defined Networks}
