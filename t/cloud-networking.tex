%pdflatex-../thesis.tex
% vim:spell spelllang=en_us

% Networking

Networking is essential part of cloud computing because it is not be possible to access any services without networking. Every service in cloud computing system is accessed via network. Network is usually also used for communication between virtual machines, migrations, storage access and for many other tasks.


% L1
First part of networking to think about is physical layer. Various Ethernet versions are used for physical layer in cloud data centers. There are many versions with different link bandwidth and wiring, but 1G and 10G with twisted pairs or optical fibers is used most widely. There were 100M called Fast Ethernet but it does not make sense to use this for server link today, because of it's limited bandwidth and almost similar price compared to 1G.

% server
It is common to insert two independent \Ac{NIC}s into every server and connect them into independent \Ac{ToR} switches, because it improves fault-tolerance. There is usually one more \Ac{NIC} for remote management and some additional cards if \Ac{SAN} is used. Remote management can be connected with detached cable or shared with any of network cards. Separate cable brings more flexibility and fault-tolerance and shared cable reduces cabling effort and thus simplifies maintenance and improves cooling effectiveness. Both solutions are used. 

% rack & ToR
About 40 servers fits into traditional rack and these server need to be connected to network infrastructure. There are different topologies, but most common is variant of (fat) tree with top-of-rack switch. There is also approach called \Uv{fabric} which implies non-blocking every-to-every mesh connection between switches. However fabric technologies are proprietary and limited to vendor.

Every rack contains about 40 servers and these servers are connected to switch called \Ac{ToR}. This switch is located in the rack and acts as access layer for servers. Servers are connected to at least two \Ac{ToR}s if additional fault-tolerance is needed. Upper network topology layers depends on data center size and scaling requirements. There can be distribution and core layer, collapsed core or some kind of fabric.

Physical topology must be adjusted to spread network layers between two or more datacenter networks. It is obviously not possible to spread physical layer, but data link layer and upper layers are possible to spread.

% L3 & address
Another view on network topology is at network layer. Internet is based on \Ac{TCP}/\Ac{IP} so it is necessary to use this protocol family and assign \Ac{IP} addresses to servers, virtual machines and other network elements. There are two different versions of \Ac{IP} protocol:
\begin{description}
	\item[version 4] is the older one, with 32 bit address space. This version is still used more than version 6 even though it's address space is depleted and new version exists for more than 15 years.
	\item[version 6] is the \Uv{new} one, uses 128 bit address space and different headers, thus it is incompatible with version 4.
\end{description}

Moder data center must provide both versions of \Ac{IP} protocol, because supporting only one versions is a huge limitation and can not be accepted for new services deployment. 
However there is a problem with obtaining \Ac{IPv4} addresses, because available pool had already been depleted and all available addreses had been divided between \Ac{RIR}s. It is beneficial to make efforts to employ \Ac{IPv6} protocol as primary one and try to limit the amount of required \Ac{IPv4} addresses.


There are different ways how to use both versions concurently:
\begin{itemize}
	\item Dual-stack is the simpliest and probably the most used solution. Each interface gets at least one \Ac{IPv4} and one \Ac{IPv6} address. Use of both versions causes additional maintenance effort because it is necessary to take care of two separate L3 networks.
	\item Tunelling \Ac{IPv6} via existing \Ac{IPv4} infrastructure with technologies like 6to4, 6rd or \Ac{ISATAP} is another way. This solution can be used for \Ac{IPv6} deployment in networks with working \Ac{IPv4}, because it is used for transmission of all packets. 
	Tunelling is usually focused on deployment in access networks, but deployment in data center network is also applicable, as described in \cite{draft-sakura-6rd}.
	\item Translating \Ac{IPv4} addresses into part of \Ac{IPv6} addressing space is different approach than previous mentinoned, because it operates on \Ac{IPv6}-only networks. This technique does not require every box to have assigned an \Ac{IPv4} address and thus is good for saving address space. Hovewer address translations may not be suitable for data center usage, because it does not preserve original \Ac{IP} addresses and makes customer tracking almost impossible. Further information can be found in \cite{ipv4-jako-sluzba}.
\end{itemize}

It is beneficial to deploy protocol \Ac{IPv6} as primary one in my opinion, because it will become more and more needed during time. Hardware on current market usually support protocol \Ac{IPv6} at least partialy, but there are still some hidden pitfalls. There may be problem for example with server's remote management, because it may not support \Ac{IPv6} and thus is totally unusable on \Ac{IPv6}-only network. None of servers I have used for practical part of this thesis have support for \Ac{IPv6} on \Ac{IPMI}. 
However there is currently only small demand and \Ac{IPv6} deployment does not bring any direct profit. Even thought there are many problems and advantages are quite hidden, it is not possible to ignore this protocol and stay with old \Ac{IPv4}.

Virtual machine migration must be taken into account durring addressing schema design, because it plays crucial role in data center. Migrations are performed between hypervisors, i.e. physical servers, and these servers may be locatet in different racks, halls or even in different data centers. It is usually required to preserve \Ac{IP} address of virtual machine during migration process and that way migration must be 

\subsection{Storage network}
% storage network

\subsection{Load balancing}
% load balancing

\subsection{Firewall}
% firewall 
