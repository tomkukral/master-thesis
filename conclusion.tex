%pdflatex-thesis.tex
% vim:spell spelllang=en_us

I have analyzed and compared networking and storage technologies used in distributed (cloud) datacenters. Virtualization is mentioned in the beginning because it is an enabling technology for the cloud computing. Cloud deployment and service models are compared and appropriate use cases are mentioned.

The biggest attention is dedicated to the cloud computing, especially to networking, storage and orchestration technologies. Comparison is made by defining the use cases and commenting advantages and disadvantages. It is not possible to rigorously decide on the best solution because there are many use cases and each of them requires individual approach.

% networking
Networking is an essential part of the datacenter design and legacy technologies are not able to fulfill current demand. Overlays techniques are supposed to provide required flexibility and network virtualization. \Ac{VXLAN}, \Ac{STT} and \Ac{NVGRE} is discussed. Hop-by-hop network virtualization is mentioned too, but it is tightly tied to the \Ac{SDN} and \Ac{SDN} is still kind of sci-fi technology because it is not widely supported.  
% load balancing
I have found out that load balancing is an important topic connected with overlay networks and \Ac{VM} migrations, so I have included the load balancing into the networking section too.

% storage

% orchestration
The orchestration is described in a theoretical part and special attention is given to OpenNebula because it is used in a practical part. I have explained principles of the orchestration with special focus on virtual networks. Contextualization packages from a practical part are presented.

I think that the virtualization brings so many improvements to the datacenters that a legacy networking technologies are not able to keep pace. Most of the technologies currently used in network layer were designed before the virtualization era and it is the reason why they are to rigid to meet nowadays requirements. It is common to live migrate the virtual machine nowadays but it was impossible in a few years before.

We are trying to build a highly agile technology, like virtualized and distributed datacenter, on top of the legacy techniques. This approach is obviously not able to work well and there are two possible possible. 

The first radical solution is to totally redesign current network stack and take current requirements into ccount. However it is very hard to design solution for all use cases with with respect to a future usage. It also does not make sense from a economical point of view because network equipment needs to be upgraded or replaced. \Ac{SDN} is, in my opinion, a typical example of this technology since it brings amazing new features and it is usually can not be used on the legacy devices because hardware changes are necessary.

Second approach is to build a new overlay network on top of an existing network. This overlay network can provide additional functionality, but also brings some limitations caused by an underlaying physical network. It is, for example, not possible to handle priority packets from the overlay networks with special care in the underlay network because the underlay knows nothing about the overlay network. Another problem is the \Ac{BUM} traffic because it is usually problematic to handle and multicast needs to be implemented in the underlay network.
The overlay networks can immediately bring some significant improvements to a datacenter networking, but there is a trade-off. I thing that overlays are an appropriate temporary solution, but I would recommend to use the hop-by-hop with \Ac{SDN} when available.

% practical
It is possible to migrate a virtual machine with really minimal outage so \Ac{VM}s can be moved between the hypervisors to optimize performance or minimize an energy consumption. It is necessary to think about a networking aspect of migration because transfer degradation will definitely appear during the migration. I have developed an application called Themis. It is capable to evaluate a virtual machine availability during live migration. It combines network measurements, orchestration and data analysis.

Typical use case is a migration of the virtual machine with \Ac{SLA}. It is necessary to measure service disruption before migration because there are the limits, for example for packet loss, defined in \Ac{SLA}. Themis can be used to migrate testing virtual machine and check whether service degradation during migration is acceptable.

A migration schema can be defined using a console or a web interface. Virtual machine availability is then evaluated according to the schema. Repetitive migrations are supported as well as configurable bandwidth for packet generator. 
The application manages virtual machines via \Ac{SSH}, starts measurement session, request migration via orchestrator \Ac{API} and collects the results. Whole process is fully automated, so no manual configuration is needed. 

Results can be viewed in a browser or exported into a \Ac{CSV} file. Sample measurement outputs are presented in appendix \ref{appendix:measurement-samples}. Source data were exported from Themis application and processed in MATLAB.

% elmag fork
Fork of the application is used in Department of Electromagnetic Field, CTU FEE, for automatic measurement of wireless links and monitoring. The migration routines are not used and the remote management is replaced by an agent responsible for a continuous packet generation. I am going to continue in development of this fork and merge new features into the main branch.
